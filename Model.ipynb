{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishNames(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        \n",
    "        self.samples = list(pd.read_table(file_path, header=None)[0])\n",
    "        self.max_len = max([len(x) for x in self.samples])\n",
    "        self.start_token = '$'\n",
    "        self.pad_token = '#'\n",
    "        #self.samples = [self.start_token + x + self.pad_token * (self.max_len - len(x)) \n",
    "        #                for x in self.samples]\n",
    "        self.samples = [self.start_token + x + self.pad_token for x in self.samples]\n",
    "        \n",
    "        self.dictionary = sorted(list(set(list(''.join(self.samples)))))\n",
    "        self.dict_size = len(self.dictionary)\n",
    "        self.dict_mapping = dict(zip(self.dictionary, np.arange(self.dict_size)))   \n",
    "        self.inv_mapping = dict(zip(np.arange(self.dict_size), self.dictionary))  \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample_string = self.samples[idx]\n",
    "        sample = [self.dict_mapping[x] for x in sample_string]\n",
    "        one_hot = np.zeros((len(sample), self.dict_size))\n",
    "        one_hot[np.arange(len(sample)), sample] = 1\n",
    "        sample_one_hot = torch.tensor(one_hot[:-1], dtype=torch.float32)\n",
    "        target = torch.tensor(sample[1:], dtype=torch.long)\n",
    "        #target_one_hot = torch.tensor(one_hot[1:], dtype=torch.float32)\n",
    "        \n",
    "        return {'name': sample_string,\n",
    "                'name_encoded': sample,\n",
    "                'sample': sample_one_hot,\n",
    "                'target': target\n",
    "               }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), -1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sample):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(len(sample['sample'])):\n",
    "        output, hidden = rnn(sample['sample'][i], hidden)\n",
    "        l = criterion(output.unsqueeze(0), sample['target'][i].unsqueeze(0))\n",
    "        loss += l\n",
    "    \n",
    "    loss /= (i+1)\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item() / len(sample['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    hidden = rnn.initHidden()\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    input_token = ' '\n",
    "    name = ''\n",
    "    \n",
    "    while input_token!='#':\n",
    "        input_token_id = mapping[input_token]\n",
    "        one_hot = np.zeros(dict_size)\n",
    "        one_hot[input_token_id] = 1\n",
    "        one_hot = torch.tensor(one_hot, dtype=torch.float32)\n",
    "    \n",
    "        output, hidden = rnn(one_hot, hidden)\n",
    "        output = softmax(output)\n",
    "        probs = output.detach().numpy()\n",
    "        input_token_id = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        input_token = inv_mapping[input_token_id]\n",
    "        name += input_token\n",
    "        \n",
    "    return name[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    # probability for capital letters being first\n",
    "    \n",
    "    hidden = rnn.initHidden()\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    input_token = ' '\n",
    "    name = ''\n",
    "    \n",
    "    input_token_id = mapping[input_token]\n",
    "    one_hot = np.zeros(dict_size)\n",
    "    one_hot[input_token_id] = 1\n",
    "    one_hot = torch.tensor(one_hot, dtype=torch.float32)\n",
    "    \n",
    "    output, hidden = rnn(one_hot, hidden)\n",
    "    output = softmax(output)\n",
    "    probs = output.detach().numpy()\n",
    "    \n",
    "    capital_letter_probs = probs[3:28].sum()\n",
    "    \n",
    "    # most probable name\n",
    "    \n",
    "    while input_token!='#':\n",
    "        input_token_id = mapping[input_token]\n",
    "        one_hot = np.zeros(dict_size)\n",
    "        one_hot[input_token_id] = 1\n",
    "        one_hot = torch.tensor(one_hot, dtype=torch.float32)\n",
    "    \n",
    "        output, hidden = rnn(one_hot, hidden)\n",
    "        output = softmax(output)\n",
    "        _, input_token_id = output.topk(1)\n",
    "        input_token_id = input_token_id.item()\n",
    "        input_token = inv_mapping[input_token_id]\n",
    "        name += input_token\n",
    "        \n",
    "    return capital_letter_probs, name[:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train loss = 0.2917\n",
      "Prob Capital Letter Being First = 0.626, Most likely name = arder\n",
      "Name samples: yiytsm,  dgliy, Dlrnflly, Ezwgem, tenson\n",
      "\n",
      "Epoch 20: Train loss = 0.2793\n",
      "Prob Capital Letter Being First = 0.693, Most likely name = arey\n",
      "Name samples: Bacr, Siavder, Hiocurus, Galg, Jodeal\n",
      "\n",
      "Epoch 30: Train loss = 0.2711\n",
      "Prob Capital Letter Being First = 0.725, Most likely name = aner\n",
      "Name samples: wbotte, Bank, Ularoes, pailot, Oedsrn\n",
      "\n",
      "Epoch 40: Train loss = 0.2667\n",
      "Prob Capital Letter Being First = 0.744, Most likely name = and\n",
      "Name samples: $Edckedy, saor, BVeerry, cCuhton, icgersyla\n",
      "\n",
      "Epoch 50: Train loss = 0.2647\n",
      "Prob Capital Letter Being First = 0.756, Most likely name = ane\n",
      "Name samples: qelles, Dverl, Waith, mor, g\n",
      "\n",
      "Epoch 60: Train loss = 0.2620\n",
      "Prob Capital Letter Being First = 0.764, Most likely name = ers\n",
      "Name samples: Fens, Qvirles, Yies, Roder, Corain\n",
      "\n",
      "Epoch 70: Train loss = 0.2596\n",
      "Prob Capital Letter Being First = 0.771, Most likely name = ard\n",
      "Name samples: manes, Gyles, ze, istlo, LeIs\n",
      "\n",
      "Epoch 80: Train loss = 0.2584\n",
      "Prob Capital Letter Being First = 0.775, Most likely name = and\n",
      "Name samples: Bley, Glens, Uanb, Fmankey, Vgley\n",
      "\n",
      "Epoch 90: Train loss = 0.2571\n",
      "Prob Capital Letter Being First = 0.779, Most likely name = arrin\n",
      "Name samples: ary, Burdin, Snegp, ray, Githerst\n",
      "\n",
      "Epoch 100: Train loss = 0.2568\n",
      "Prob Capital Letter Being First = 0.781, Most likely name = arey\n",
      "Name samples: fford, Asrey, Osby, Portor, Essen\n",
      "\n",
      "Epoch 110: Train loss = 0.2555\n",
      "Prob Capital Letter Being First = 0.783, Most likely name = ane\n",
      "Name samples: gaywer, Kton, Ierlind, Hondon, Kind\n",
      "\n",
      "Epoch 120: Train loss = 0.2547\n",
      "Prob Capital Letter Being First = 0.785, Most likely name = arden\n",
      "Name samples: Kron, Lerier, Neishan, Dawoes, Haidey\n",
      "\n",
      "Epoch 130: Train loss = 0.2538\n",
      "Prob Capital Letter Being First = 0.786, Most likely name = arrin\n",
      "Name samples:  orsey, rterinn, Jawry, Youdword, Esmy\n",
      "\n",
      "Epoch 140: Train loss = 0.2538\n",
      "Prob Capital Letter Being First = 0.787, Most likely name = are\n",
      "Name samples: Cos, Lanle, Ensopsin, Ilf, jor\n",
      "\n",
      "Epoch 150: Train loss = 0.2535\n",
      "Prob Capital Letter Being First = 0.787, Most likely name = ard\n",
      "Name samples: Aston, Peay, toce, Neab, Egson\n",
      "\n",
      "Epoch 160: Train loss = 0.2530\n",
      "Prob Capital Letter Being First = 0.787, Most likely name = arring\n",
      "Name samples: man, Amsens, Micole, Dann, Broon\n",
      "\n",
      "Epoch 170: Train loss = 0.2529\n",
      "Prob Capital Letter Being First = 0.787, Most likely name = arring\n",
      "Name samples: ordy, Liggrn, Donns, shcold, Ninstl\n",
      "\n",
      "Epoch 180: Train loss = 0.2523\n",
      "Prob Capital Letter Being First = 0.787, Most likely name = arrin\n",
      "Name samples: Qerr, yras, Pacotl, vinetr, delle\n",
      "\n",
      "Epoch 190: Train loss = 0.2515\n",
      "Prob Capital Letter Being First = 0.787, Most likely name = arrin\n",
      "Name samples: Elmf, Wille, ilby, Elton, vanst\n",
      "\n",
      "Epoch 200: Train loss = 0.2511\n",
      "Prob Capital Letter Being First = 0.786, Most likely name = arrin\n",
      "Name samples: Be, Rogst, burt, Roxnill, Kdey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {'data_path': 'data/English.txt',\n",
    "          'l_rate': 0.01,\n",
    "          'n_epochs': 200,\n",
    "          'n_per_epoch': 3000,\n",
    "          'n_hidden_features': 128}\n",
    "\n",
    "dataset = EnglishNames(file_path=config['data_path'])\n",
    "dict_size = dataset.dict_size\n",
    "inv_mapping = dataset.inv_mapping\n",
    "mapping = dataset.dict_mapping\n",
    "\n",
    "rnn = RNN(dict_size, config['n_hidden_features'], dict_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = config['l_rate']\n",
    "\n",
    "for epoch in range(config['n_epochs']):\n",
    "    epoch_loss = 0\n",
    "    indices = np.random.choice(np.arange(dataset.__len__()), \n",
    "                               config['n_per_epoch'], replace=False)\n",
    "    \n",
    "    for i in indices:\n",
    "        sample = dataset.__getitem__(i)\n",
    "        output, loss = train(sample)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        epoch_loss /= config['n_per_epoch']\n",
    "        checking = check()\n",
    "        print('Epoch {}: Train loss = {:.4f}'.format(epoch+1, epoch_loss))\n",
    "        print('Prob Capital Letter Being First = {:.3f}, Most likely name = {}'\n",
    "              .format(checking[0], checking[1]))\n",
    "        print('Name samples: {}, {}, {}, {}, {}\\n'.format(generate(), generate(), generate(), \n",
    "                                                  generate(), generate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
